{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\" , \"french\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 14064: expected 1 fields, saw 2\\nSkipping line 14065: expected 1 fields, saw 2\\nSkipping line 14066: expected 1 fields, saw 2\\nSkipping line 14067: expected 1 fields, saw 2\\nSkipping line 14068: expected 1 fields, saw 2\\nSkipping line 14069: expected 1 fields, saw 2\\nSkipping line 14071: expected 1 fields, saw 2\\nSkipping line 14072: expected 1 fields, saw 2\\nSkipping line 14073: expected 1 fields, saw 2\\nSkipping line 14074: expected 1 fields, saw 2\\nSkipping line 49361: expected 1 fields, saw 7\\nSkipping line 49362: expected 1 fields, saw 2\\nSkipping line 49363: expected 1 fields, saw 2\\nSkipping line 49364: expected 1 fields, saw 5\\n'\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table('C:/Users/znader/Desktop/Python/Chat Data/AndreLeb_chat.txt' , error_bad_lines=False , header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['chat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = data['chat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descciption of the case is the one to take into account to get key words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate all the rows for the same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = desc.str.cat(sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "desc = re.sub(r'\\d+', '', desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = set(stopwords.words('english')) \n",
    "stop_words_fr = set(stopwords.words('french')) \n",
    "stopwords = (stop_words_en ,stop_words_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that removes all the stowords in english word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords_fr(word_list):\n",
    "        processed_word_list = []\n",
    "        for word in word_list:\n",
    "            word = word.lower() # in case they arenet all lower cased\n",
    "            if word not in  stop_words_fr:\n",
    "                processed_word_list.append(word)\n",
    "        return processed_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords_en(word_list):\n",
    "        processed_word_list = []\n",
    "        for word in word_list:\n",
    "            word = word.lower() # in case they arenet all lower cased\n",
    "            if word not in  stop_words_en:\n",
    "                processed_word_list.append(word)\n",
    "        return processed_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = ('andré' ,'pm' , 'b' , 'charly', 'hallak' , 'w' , 'h' , 'al' , 'p' ,  'audio', 'v' , 'aw' , 'ok' , 'oki' , 'dany' , 'ra' ,   'danyou' , 'oui', 'ana', 'ca', 'akid', 'va', 'chou', 'eh', 'pa', 'ziad' , 'oursonne' , 'shu', 'chi' , 'lol' , 'omitted', 'u', 'bass', 'image', 'bs', 'we', 'el', 'dit', 'di' ,'enno','joe', 'ghanem', 'fi', 'hek','chez', 'fait', 'si', 'les')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_chat_words(word_list):\n",
    "        processed_word_list = []\n",
    "        for word in word_list:\n",
    "            word = word.lower() # in case they arenet all lower cased\n",
    "            if word not in  chat_words:\n",
    "                processed_word_list.append(word)\n",
    "        return processed_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will take words , remove the stop workds and stem them by removing and rooting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_words(word_list):\n",
    "    new = []\n",
    "    desc_new = tokenizer.tokenize(word_list)\n",
    "    desc_new_1 = remove_stopwords_en(desc_new)\n",
    "    desc_new_2 = remove_stopwords_fr(desc_new_1)\n",
    "    desc_new_3 = remove_stopwords_chat_words(desc_new_2)\n",
    "    \n",
    "    return desc_new_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bro', 'men', 'eza', 'bel', 'bas', 'enta', 'man', 'lek', 'ktir', 'ya', 'bi', 'badak', 'deal', 'kamen', 'hal', 'yalla', 'home', 'badde', 'ken', 'bkra', 'lal', 'mech', 'tamem', 'hayda', 'kel', 'kif', 'barke', 'broo', 'bet', 'lyom', 'chouf', 'kenet', 'rah', 'yala', 'fik', 'lezem', 'okay', 'walla', 'hon', 'sawa']\n"
     ]
    }
   ],
   "source": [
    "word_counter = {}\n",
    "for word in adjust_words(desc):\n",
    "    if word in word_counter:\n",
    "        word_counter[word] += 1\n",
    "    else:\n",
    "        word_counter[word] = 1\n",
    "\n",
    "popular_words = sorted(word_counter, key = word_counter.get, reverse = True)\n",
    "\n",
    "print(popular_words[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numero collé , il faut le garder car en libanais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
